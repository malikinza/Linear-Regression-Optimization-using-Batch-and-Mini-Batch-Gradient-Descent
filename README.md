# Linear-Regression-using-Batch-and-Mini-Batch-Gradient-Descent

Gradient descent is an iterative optimization algorithm for finding the local minimum of a diffrentiable function. In machine learning, it is used to find a function's parameters that minimize the cost function. In this project, I applied the gradient descent algorithm to a linear regression model with a fixed learning rate to find optimal values 

This project aims to find parameters of a linear regression model that results in the minimum test error and batch time.  

